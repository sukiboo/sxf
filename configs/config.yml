exp_name: null
seed: 2024
num_steps: 100000
ckpt_step: 1000

params_env:

    params_state:
        dim: 100
        low: -1
        high: 1
        dist: uniform
    params_action:
        num: 10
        dim: 100
        low: -1
        high: 1
        dist: uniform
    params_feedback:
        arch_s: [64]
        arch_a: [64]
        dim_feature: 16
        relevance: cossim
    params_reward:
        reward_type: scale
        params_cont: {scale: 1, interval: [-1,1]}
        params_disc: {values: [1,-1], default: 0}


params_agent:

  - name: ffnn_pg
    params_arch:
        arch_type: ffnn
        arch: [512,256,32]
        activation: relu
    params_loss:
        loss_type: pg
        opt_alg: {name: Adam, learning_rate: 0.001, beta_1: 0.9, beta_2: 0.999}
        regularization: {entropy: .01, l2: .01}
    batch_size: 32
    temperature: 1.

  - name: ffnn_q
    params_arch:
        arch_type: ffnn
        arch: [512,256,32]
        activation: relu
    params_loss:
        loss_type: q
        opt_alg: {name: Adam, learning_rate: 0.001, beta_1: 0.9, beta_2: 0.999}
        regularization: {entropy: .001, l2: .01}
    batch_size: 32
    temperature: 10.

  - name: drrn_pg
    params_arch:
        arch_type: drrn
        arch: [256,256,32]
        activation: tanh
        relevance: inner
    params_loss:
        loss_type: pg
        opt_alg: {name: Adam, learning_rate: 0.001, beta_1: 0.9, beta_2: 0.999}
        regularization: {entropy: .01, l2: .01}
    batch_size: 32
    temperature: 1.

  - name: drrn_q
    params_arch:
        arch_type: drrn
        arch: [256,256,32]
        activation: tanh
        relevance: inner
    params_loss:
        loss_type: q
        opt_alg: {name: Adam, learning_rate: 0.001, beta_1: 0.9, beta_2: 0.999}
        regularization: {entropy: .001, l2: .01}
    batch_size: 32
    temperature: 10.

